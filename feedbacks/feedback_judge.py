import logging
import os
import sys

_current_dir = os.path.dirname(os.path.abspath(__file__))
_parent_dir = os.path.dirname(_current_dir)
sys.path.extend([_current_dir, _parent_dir])

from common import vlm
from common.basics import PromptBuilder
from common.utils import colorstring

# Configure logging
logging.basicConfig(
    level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s"
)


class FeedbackJudge(vlm.VisionLanguageModel):
    def __init__(
        self,
        api_key,
        api_base_url,
        model_name,
        org_id,
        prompt_templates,
        **model_kwargs,
    ):
        super().__init__(
            api_key,
            api_base_url,
            model_name,
            org_id,
            **model_kwargs,
        )
        self.prompt_templates = prompt_templates

    def is_feedback_better(
        self, new_feedback: str, old_feedback: str, design_goal: str
    ) -> bool:
        """
        Use the LLM to determine if the current iteration's design is better than the previous iteration's design
        based on their feedback.

        :param new_feedback: The feedback for the current iteration.
        :param old_feedback: The feedback for the previous iteration.
        :param design_goal: The design goal to compare against.
        :return: True if the current iteration is better, otherwise False.
        """
        # Construct the prompt
        prompt = self.prompt_templates["is_feedback_better"]["prompt_template"].format(
            design_goal=design_goal,
            old_feedback=old_feedback,
            new_feedback=new_feedback,
        )

        pb = PromptBuilder()
        pb.add_system_prompt(
            self.prompt_templates["is_feedback_better"]["system_prompt"]
        )
        pb.add_user_prompt(prompt)

        # Query the LLM
        response = self.query_with_promptbuilder(pb)

        # Parse the Markdown-formatted response
        decision = self._parse_markdown_response(response, key="Decision")
        return decision.lower() == "yes"

    def is_design_goal_achieved(
        self, feedback: str, design_goal: str
    ) -> tuple[bool, float]:
        """
        Use the LLM to determine if the design goal has been achieved and provide a regression score.

        :param feedback: The feedback generated by the VLM.
        :param design_goal: The design goal to compare against.
        :return: A tuple containing a boolean indicating if the goal is achieved and a regression score.
        """
        # Construct the prompt
        prompt = self.prompt_templates["is_design_goal_achieved"][
            "prompt_template"
        ].format(
            design_goal=design_goal,
            feedback=feedback,
        )

        pb = PromptBuilder()
        pb.add_system_prompt(
            self.prompt_templates["is_design_goal_achieved"]["system_prompt"]
        )
        pb.add_user_prompt(prompt)

        # Query the LLM
        response = self.query_with_promptbuilder(pb)
        logging.info(
            colorstring(f"Feedback Judge determines that:\n{response}", "cyan")
        )

        # Parse the Markdown-formatted response
        achieved = (
            self._parse_markdown_response(response, key="Achieved").lower() == "yes"
        )
        score = float(self._parse_markdown_response(response, key="Score"))

        return achieved, score

    def _parse_markdown_response(self, response: str, key: str) -> str:
        """
        Parse a Markdown-formatted response to extract the value for a specific key.
        Strips all Markdown formatting (e.g., **, *, _) from the extracted value.

        :param response: The Markdown-formatted response.
        :param key: The key to extract (e.g., "Decision", "Score").
        :return: The value associated with the key, stripped of Markdown formatting.
        """
        import re

        # Find the line containing the key
        match = re.search(rf"{key}:\s*([^\n]+)", response, re.IGNORECASE)
        if match:
            # Extract the value and strip all Markdown formatting (e.g., **, *, _)
            value = match.group(1).strip()
            value = re.sub(r"[*_]", "", value)  # Remove all * and _ characters
            return value
        raise ValueError(f"Key '{key}' not found in response.")


# # Example usage
# # Initialize the FeedbackJudge
# feedback_judge = FeedbackJudge(
#     config["api_key"],
#     config.get("api_base_url"),
#     config.get("model_name", "gpt-4"),
#     config.get("org_id"),
#     prompt_templates,
#     **config.get("model_kwargs", {}),
# )

# # Compare feedback to determine if the current iteration is better
# is_better = feedback_judge.is_feedback_better(new_feedback, old_feedback, design_goal)

# # Evaluate design goal achievement
# is_achieved, score = feedback_judge.is_design_goal_achieved(feedback, design_goal)

# print(f"Is current iteration better? {is_better}")
# print(f"Is design goal achieved? {is_achieved} (Score: {score})")
